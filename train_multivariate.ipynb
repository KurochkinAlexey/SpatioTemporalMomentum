{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b6afc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import random\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from empyrical import sharpe_ratio\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from src.features import DeepMomentumFeatures, MACDFeatures, DatetimeFeatures, DefaultFeatureCreator\n",
    "from src.data import MultivariateTrainValTestSplitter\n",
    "from src.utils import reg_l1, reg_turnover, sharpe_loss\n",
    "from src.simple_models import *\n",
    "from src.tft import TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e46bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b5e107",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train models as described in https://arxiv.org/pdf/2302.10175.pdf\n",
    "# Each asset is used as feature, data shape is (asset_length, lookback_window, n_assets*n_features_per_asset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7d12f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009fa4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sectors_dict.pickle', 'rb') as f:\n",
    "    sectors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdde5f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select assets to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db1873b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "N_ASSETS_PER_SECTOR = 5\n",
    "LENGTH_THRESHOLD = 3000\n",
    "DATASET_DIRNAME = 'yf_data'\n",
    "USE_ADJUSTED_CLOSE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b9f0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "all_dfs = []\n",
    "all_used_assets = []\n",
    "for current_sector in sectors.keys():\n",
    "    dfs = []\n",
    "    assets = []\n",
    "    for asset in sectors[current_sector]:\n",
    "        df = pd.read_csv(os.path.join(DATASET_DIRNAME, f'{current_sector}', f'{asset}.csv'))\n",
    "        # use only stocks with long enough history\n",
    "        if len(df) > LENGTH_THRESHOLD:\n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "            \n",
    "            close_col = 'Adj Close' if USE_ADJUSTED_CLOSE else 'Close'\n",
    "            \n",
    "            cols = ['Date', 'Open', 'High', 'Low', close_col, 'Volume']\n",
    "            \n",
    "            df = df[cols]\n",
    "            \n",
    "            df.columns = ['Date'] + ['{}_{}'.format(asset, name) for name in \\\n",
    "                                     ['open', 'high', 'low', 'close', 'volume']]\n",
    "            dfs.append(df)\n",
    "            assets.append(asset)\n",
    "\n",
    "    df = dfs[0]\n",
    "    for i in range(1, len(dfs)):\n",
    "        df = pd.merge(df, dfs[i], left_on='Date', right_on='Date', how='inner')\n",
    "\n",
    "    used_assets = np.random.choice(assets, N_ASSETS_PER_SECTOR, replace=False)\n",
    "    df = df[['Date'] + [name for name in df.columns[1:] if name.split('_')[0] in used_assets]]\n",
    "    all_dfs.append(df)\n",
    "    all_used_assets.extend(list(used_assets))\n",
    "\n",
    "df = all_dfs[0]\n",
    "for i in range(1, len(all_dfs)):\n",
    "    df = pd.merge(df, all_dfs[i], left_on='Date', right_on='Date', how='inner')\n",
    "df = df.set_index(df['Date'])\n",
    "print('loaded dataframe shape', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e8edb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e724677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f66d9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_DATETIME_FEATURES = False #categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22d72a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_features = [DeepMomentumFeatures, MACDFeatures] # one can implement and add other features like VWAP, RSI etc\n",
    "features_configs = [{}, {}]\n",
    "if USE_DATETIME_FEATURES:\n",
    "    used_features.append(DatetimeFeatures)\n",
    "    features_configs.append({})\n",
    "\n",
    "fc = DefaultFeatureCreator(df, all_used_assets, used_features, features_configs)\n",
    "\n",
    "features = fc.create_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e02fc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if something is wrong with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8323dead",
   "metadata": {},
   "outputs": [],
   "source": [
    "BAD_VALUES_THRESHOLD = 10000\n",
    "for key in features.keys():\n",
    "    assert features[key].isnull().sum().sum() == 0\n",
    "    assert features[key].max().max() < BAD_VALUES_THRESHOLD\n",
    "    assert features[key].min().min() > -BAD_VALUES_THRESHOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d821280",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_use = ['norm_daily_return',\n",
    "               'norm_monthly_return',\n",
    "               'norm_quarterly_return',\n",
    "               'norm_biannual_return',\n",
    "               'norm_annual_return',\n",
    "               'macd_8_24',\n",
    "               'macd_16_48',\n",
    "               'macd_32_96']\n",
    "\n",
    "if USE_DATETIME_FEATURES:\n",
    "    datetime_cols = ['day_of_week', 'day_of_month', 'month_of_year']\n",
    "    \n",
    "    # encode categorical features\n",
    "    for col in datetime_cols:\n",
    "        if key in features.keys():\n",
    "            features[key][col] = LabelEncoder().fit_transform(features[key][col])\n",
    "else:\n",
    "    datetime_cols = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb6eaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# available models\n",
    "MODEL_MAPPING = {'lstm': LSTMnet,\n",
    "                 'slp': SLP,\n",
    "                 'mlp': MLP,\n",
    "                 'conv': TCN,\n",
    "                 'tft': TFT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4060ca8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a8da17",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling = None #'standard', 'minmax'\n",
    "model_type = 'mlp' # model type from MODEL_MAPPING\n",
    "if model_type == 'tft':\n",
    "    history_size = 63\n",
    "    encoder_length = 42 # in case of tft encoder length should be int\n",
    "    model_params = {'device': 'cuda'}\n",
    "else:\n",
    "    history_size = 21\n",
    "    encoder_length = None # in case of non tft model encoder length should be None\n",
    "    model_params = {}\n",
    "    \n",
    "# training is done via expanding window approach:\n",
    "# train, val, test = (2010-2017, 2017-2018, 2018-2019), then (2010-2018, 2018-2019, 2019-2020) etc  \n",
    "val_delta = pd.Timedelta('365days')\n",
    "test_delta = pd.Timedelta('365days')\n",
    "date_range = pd.date_range('2017-01-01', '2023-12-31', freq='365d')\n",
    "\n",
    "apply_turnover_reg = False\n",
    "apply_l1_reg = False\n",
    "weight_decay = 1e-5\n",
    "lr = 1e-3\n",
    "decay_steps = 10\n",
    "decay_gamma = 0.75\n",
    "early_stopping_rounds = 10\n",
    "n_epochs = 2\n",
    "device = 'cuda'\n",
    "target_vol = 0.15 #measure for turnover evaluation\n",
    "basis_points = [0, 1, 5, 10] #coefficients for turnover evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df534b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_seed(seed):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "    #torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11155c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in [42]:\n",
    "    \n",
    "    if not os.path.exists('weights'):\n",
    "        os.mkdir('weights')\n",
    "    if not os.path.exists('results'):\n",
    "        os.mkdir('results')\n",
    "    \n",
    "    test_dts = []\n",
    "    results = {}\n",
    "    splitter = MultivariateTrainValTestSplitter(features, cols_to_use, datetime_cols,\n",
    "                                                'target_returns', 'target_returns_nonscaled',\n",
    "                                                'daily_vol', scaling=scaling, timesteps=history_size,\n",
    "                                                 encoder_length=encoder_length)\n",
    "    for start in date_range:\n",
    "        train_loader, val_loader, test_loader, test_dt, cat_info = splitter.split(start, val_delta,\n",
    "                                                                                  test_delta, seed)\n",
    "        test_dts.append(test_dt)\n",
    "        if len(test_loader) == 0:\n",
    "            continue\n",
    "        \n",
    "        dt = start\n",
    "        results[dt] = {}\n",
    "        \n",
    "        # in case of tft number of model inputs is different from other model types\n",
    "        batch_data = next(iter(train_loader))\n",
    "        if model_type != 'tft':\n",
    "            batch_x, batch_y, _, _ = batch_data\n",
    "        else:\n",
    "            batch_x, _, _, _, _, _, batch_y, _, _ = batch_data\n",
    "        \n",
    "        input_dim = batch_x.shape[2]\n",
    "        output_dim = batch_y.shape[2]\n",
    "        timesteps = history_size\n",
    "        \n",
    "        # fix weights initialization for reproducibility\n",
    "        _set_seed(seed)\n",
    "        model = MODEL_MAPPING[model_type](input_dim, output_dim,\n",
    "                                          timesteps, cat_info=cat_info, **model_params).to(device)\n",
    "        \n",
    "        opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        sc = torch.optim.lr_scheduler.StepLR(opt, decay_steps, decay_gamma)\n",
    "\n",
    "        counter = 0\n",
    "        \n",
    "        train_losses = []\n",
    "        train_l1_losses = []\n",
    "        train_turnover_losses = []\n",
    "        val_losses = []\n",
    "        val_turnover_losses = []\n",
    "        best_val_sharpe = np.NINF\n",
    "        \n",
    "        for e in tqdm(range(n_epochs)):\n",
    "            train_loss = 0\n",
    "            train_l1_loss = 0\n",
    "            train_turnover_loss = 0\n",
    "            model.train()\n",
    "            for batch_data in tqdm(train_loader):\n",
    "                # unpack batches\n",
    "                for i in range(len(batch_data)):\n",
    "                    batch_data[i] = batch_data[i].to(device)\n",
    "                \n",
    "                if model_type != 'tft':\n",
    "                    batch_x, batch_y, batch_y_orig, batch_vol = batch_data\n",
    "                    input_data = [batch_x]\n",
    "                else:\n",
    "                    batch_x_enc_real, batch_x_enc_cat, batch_x_dec_real, batch_x_dec_cat, \\\n",
    "                    batch_enc_len, batch_dec_len, batch_y, batch_y_orig, batch_vol = batch_data\n",
    "                    input_data = [batch_x_enc_real, batch_x_enc_cat, batch_x_dec_real, batch_x_dec_cat, \\\n",
    "                                  batch_enc_len, batch_dec_len]\n",
    "                # train step\n",
    "                opt.zero_grad()\n",
    "\n",
    "                output = model(*input_data)\n",
    "\n",
    "                l = sharpe_loss(output, batch_y)\n",
    "                train_loss += l.item()\n",
    "                \n",
    "                # optionally apply regularization\n",
    "                if apply_l1_reg:\n",
    "                    l_l1 = reg_l1(model)\n",
    "                    train_l1_loss += l_l1.item()\n",
    "                    l += l_l1\n",
    "\n",
    "                if apply_turnover_reg:\n",
    "                    l_turnover = reg_turnover(output, batch_vol)\n",
    "                    train_turnover_loss += l_turnover.item()\n",
    "                    l += l_turnover\n",
    "                \n",
    "                l.backward()\n",
    "                opt.step()\n",
    "            \n",
    "            # we do not want learning rate to be too small\n",
    "            if sc.get_last_lr()[0] > 1e-5:\n",
    "                sc.step()\n",
    "            \n",
    "            val_loss = 0\n",
    "            val_turnover_loss = 0\n",
    "            \n",
    "            preds = []\n",
    "            returns = []\n",
    "            vols = []\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            #evaluate model performance on validation dataset\n",
    "            with torch.no_grad():\n",
    "                for batch_data in tqdm(val_loader):\n",
    "                    # unpack batches\n",
    "                    for i in range(len(batch_data)):\n",
    "                        batch_data[i] = batch_data[i].to(device)\n",
    "\n",
    "                    if model_type != 'tft':\n",
    "                        batch_x, batch_y, batch_y_orig, batch_vol = batch_data\n",
    "                        input_data = [batch_x]\n",
    "                    else:\n",
    "                        batch_x_enc_real, batch_x_enc_cat, batch_x_dec_real, batch_x_dec_cat, \\\n",
    "                        batch_enc_len, batch_dec_len, batch_y, batch_y_orig, batch_vol = batch_data\n",
    "                        input_data = [batch_x_enc_real, batch_x_enc_cat, batch_x_dec_real, batch_x_dec_cat, \\\n",
    "                                      batch_enc_len, batch_dec_len]\n",
    "                    \n",
    "                    output = model(*input_data)\n",
    "                    \n",
    "                    l = sharpe_loss(output, batch_y)\n",
    "                    val_loss += l.item()\n",
    "\n",
    "                    if apply_turnover_reg:\n",
    "                        l_turnover = reg_turnover(output, batch_vol)\n",
    "                        val_turnover_loss += l_turnover.item()\n",
    "                    \n",
    "                    # select last timestep as we no longer need for time axis in batch, collect data\n",
    "                    returns.append(batch_y[:, -1, :].detach().cpu().numpy())\n",
    "                    preds.append(output[:, -1, :].detach().cpu().numpy())\n",
    "                    vols.append(batch_vol[:, -1, :].detach().cpu().numpy())\n",
    "                    \n",
    "            # create tensors from collected batches of data\n",
    "            preds = np.concatenate(preds)\n",
    "            returns = np.concatenate(returns)\n",
    "            vols = np.concatenate(vols)\n",
    "            \n",
    "            #annualized volatility\n",
    "            vols = vols * 252**0.5\n",
    "            # validation turnover\n",
    "            T = target_vol*np.abs(np.diff(preds/(vols+1e-12), prepend=0.0, axis=0))\n",
    "            # validation sharpe ratios with different turnover strength\n",
    "            val_sharpes = {}\n",
    "            \n",
    "            # calculate Sharpe Ratio given different turnover strength\n",
    "            for c in basis_points:\n",
    "                captured = returns*preds - 1e-4*c*T\n",
    "                R = np.mean(captured, axis=1)\n",
    "                sharpes = sharpe_ratio(R)\n",
    "                sharpes = np.mean(sharpes)\n",
    "                val_sharpes[c] = sharpes\n",
    "                \n",
    "            # one can use sharpe ratio averaged by all turnover coefficients as validation performance metric\n",
    "            #val_sharpe = np.mean(list(val_sharpes.values()))\n",
    "            \n",
    "            #select \"pure\" Sharpe Ratio as current epoch metric\n",
    "            val_sharpe = val_sharpes[0]\n",
    "            \n",
    "            # if current metric is best, save model weights\n",
    "            if best_val_sharpe < val_sharpe and e > 0:\n",
    "                best_val_sharpe = val_sharpe\n",
    "                counter = 0\n",
    "                torch.save(model.state_dict(), os.path.join('weights', '{}_seed_{}.pt'.format(model_type, seed)))\n",
    "            \n",
    "            else:\n",
    "                counter += 1\n",
    "            \n",
    "            # if metric value didn't improve for several epochs, stop training\n",
    "            if counter > early_stopping_rounds:\n",
    "                break\n",
    "            \n",
    "            # aggregate losses and metrics, print current epoch training state\n",
    "            train_loss /= len(train_loader)\n",
    "            train_l1_loss/= len(train_loader)\n",
    "            train_turnover_loss /= len(train_loader)\n",
    "            val_loss /= len(val_loader)\n",
    "            val_turnover_loss /= len(val_loader)\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            train_l1_losses.append(train_l1_loss)\n",
    "            train_turnover_losses.append(train_turnover_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            val_turnover_losses.append(val_turnover_loss)\n",
    "            \n",
    "            print('Iter: ', e)\n",
    "            print('Train loss: ', round(train_losses[-1], 3))\n",
    "            print('Val loss: ', round(val_losses[-1], 3))\n",
    "            print('Validation Sharpe Ratio')\n",
    "            for key in val_sharpes.keys():\n",
    "                print('C: ', key, 'SR: ', round(val_sharpes[key], 3))\n",
    "            if apply_l1_reg:\n",
    "                print('L1 loss', round(train_l1_losses[-1], 5))\n",
    "            if apply_turnover_reg:\n",
    "                print('Train turnover loss: ', round(train_turnover_losses[-1], 5))\n",
    "                print('Val turnover loss: ', round(val_turnover_losses[-1], 5))\n",
    "            print('Epochs till end: ', early_stopping_rounds - counter + 1)\n",
    "            print()\n",
    "        \n",
    "        # plot losses evolution during training\n",
    "        print('Validation dates: ', start, start+val_delta)\n",
    "        \n",
    "        plt.figure(figsize=(20, 10))\n",
    "        plt.title('Loss evolution')\n",
    "        #plt.plot(train_losses, label='train', marker='o')\n",
    "        plt.plot(val_losses, label='validation', marker='o')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        if apply_l1_reg:\n",
    "            plt.figure(figsize=(20, 10))\n",
    "            plt.title('L1 regularization loss evolution')\n",
    "            plt.plot(train_l1_losses, label='train', marker='o')\n",
    "            plt.ylabel('L1 Loss')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        \n",
    "        if apply_turnover_reg:\n",
    "            plt.figure(figsize=(20, 10))\n",
    "            plt.title('Turnover loss evolution')\n",
    "            plt.plot(train_turnover_losses, label='train', marker='o')\n",
    "            plt.plot(val_turnover_losses, label='validation', marker='o')\n",
    "            plt.ylabel('Turnover loss')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        \n",
    "        # load best checkpoint in terms of sharpe ratio on validation dataset\n",
    "        model.load_state_dict(torch.load(os.path.join('weights', '{}_seed_{}.pt'.format(model_type, seed))))\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        val_preds = []\n",
    "        val_returns = []\n",
    "        val_returns_orig = []\n",
    "        val_vols = []\n",
    "\n",
    "        model.eval()\n",
    "        \n",
    "        # calculate model predictions on validation and test datasets\n",
    "        with torch.no_grad():\n",
    "            for batch_data in val_loader:\n",
    "                for i in range(len(batch_data)):\n",
    "                    batch_data[i] = batch_data[i].to(device)\n",
    "                \n",
    "                if model_type != 'tft':\n",
    "                    batch_x, batch_y, batch_y_orig, batch_vol = batch_data\n",
    "                    input_data = [batch_x]\n",
    "                else:\n",
    "                    batch_x_enc_real, batch_x_enc_cat, batch_x_dec_real, batch_x_dec_cat, \\\n",
    "                    batch_enc_len, batch_dec_len, batch_y, batch_y_orig, batch_vol = batch_data\n",
    "                    input_data = [batch_x_enc_real, batch_x_enc_cat, batch_x_dec_real, batch_x_dec_cat, \\\n",
    "                                  batch_enc_len, batch_dec_len]\n",
    "\n",
    "                output = model(*input_data)\n",
    "\n",
    "\n",
    "                # select last timestep as we no longer need for time axis in batch\n",
    "                val_returns.append(batch_y[:, -1, :].detach().cpu().numpy())\n",
    "                val_returns_orig.append(batch_y_orig[:, -1, :].detach().cpu().numpy())\n",
    "                val_preds.append(output[:, -1, :].detach().cpu().numpy())\n",
    "                val_vols.append(batch_vol[:, -1, :].detach().cpu().numpy())\n",
    "\n",
    "\n",
    "        val_preds = np.concatenate(val_preds)\n",
    "        val_returns = np.concatenate(val_returns)\n",
    "        # one can evaluate model performance using returns which were not scaled by volatility\n",
    "        val_returns_orig = np.concatenate(val_returns_orig) \n",
    "        val_vols = np.concatenate(val_vols)\n",
    "        \n",
    "        test_preds = []\n",
    "        test_returns = []\n",
    "        test_returns_orig = []\n",
    "        test_vols = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_data in test_loader:\n",
    "                for i in range(len(batch_data)):\n",
    "                    batch_data[i] = batch_data[i].to(device)\n",
    "                \n",
    "                if model_type != 'tft':\n",
    "                    batch_x, batch_y, batch_y_orig, batch_vol = batch_data\n",
    "                    input_data = [batch_x]\n",
    "                else:\n",
    "                    batch_x_enc_real, batch_x_enc_cat, batch_x_dec_real, batch_x_dec_cat, \\\n",
    "                    batch_enc_len, batch_dec_len, batch_y, batch_y_orig, batch_vol = batch_data\n",
    "                    input_data = [batch_x_enc_real, batch_x_enc_cat, batch_x_dec_real, batch_x_dec_cat, \\\n",
    "                                  batch_enc_len, batch_dec_len]\n",
    "\n",
    "                output = model(*input_data)\n",
    "\n",
    "\n",
    "                # select last timestep as we no longer need for time axis in batch\n",
    "                test_returns.append(batch_y[:, -1, :].detach().cpu().numpy())\n",
    "                test_returns_orig.append(batch_y_orig[:, -1, :].detach().cpu().numpy())\n",
    "                test_preds.append(output[:, -1, :].detach().cpu().numpy())\n",
    "                test_vols.append(batch_vol[:, -1, :].detach().cpu().numpy())\n",
    "\n",
    "\n",
    "        test_preds = np.concatenate(test_preds)\n",
    "        test_returns = np.concatenate(test_returns)\n",
    "        # one can evaluate model performance using returns which were not scaled by volatility\n",
    "        test_returns_orig = np.concatenate(test_returns_orig)\n",
    "        test_vols = np.concatenate(test_vols)\n",
    "        \n",
    "        # collect predictions and data for current window\n",
    "        \n",
    "        results[dt]['val'] = {}\n",
    "        results[dt]['test'] = {}\n",
    "        results[dt]['test_dt'] = test_dt\n",
    "        \n",
    "        results[dt]['val']['preds'] = val_preds\n",
    "        results[dt]['val']['returns'] = val_returns\n",
    "        results[dt]['val']['returns_orig'] = val_returns_orig\n",
    "        results[dt]['val']['vols'] = val_vols\n",
    "        \n",
    "        results[dt]['test']['preds'] = test_preds\n",
    "        results[dt]['test']['returns'] = test_returns\n",
    "        results[dt]['test']['returns_orig'] = test_returns_orig\n",
    "        results[dt]['test']['vols'] = test_vols\n",
    "        \n",
    "    # dump results of experiment\n",
    "    with open(os.path.join('results', '{}_seed_{}.pickle'.format(model_type, seed)), 'wb') as f:\n",
    "        pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88d665c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
